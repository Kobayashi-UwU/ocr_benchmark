{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_check(cleaned_answer,cleaned_input):\n",
    "    distance = Levenshtein.distance(cleaned_answer, cleaned_input)\n",
    "    max_len = max(len(cleaned_answer), len(cleaned_input))\n",
    "    similarity_ratio = 1 - (distance / max_len)\n",
    "\n",
    "    return similarity_ratio\n",
    "\n",
    "def word_error_rate(ground_truth, recognized_text):\n",
    "    return Levenshtein.distance(ground_truth, recognized_text) / len(ground_truth.split())\n",
    "\n",
    "def character_error_rate(ground_truth, recognized_text):\n",
    "    return Levenshtein.distance(ground_truth, recognized_text) / len(ground_truth)\n",
    "\n",
    "def precision_recall_f1(ground_truth, recognized_text):\n",
    "    true_positives = sum([1 for token in recognized_text.split() if token in ground_truth])\n",
    "    false_positives = max(len(recognized_text.split()) - true_positives, 0)\n",
    "    false_negatives = max(len(ground_truth.split()) - true_positives, 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def levenshtein_distance(ground_truth, recognized_text):\n",
    "    return Levenshtein.distance(ground_truth, recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_without(output_csv_path,path_answer,path_input):\n",
    "    # Open the CSV file in write mode\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header row\n",
    "        csvwriter.writerow(['Image', 'Word Error Rate', 'Character Error Rate', 'Precision', 'Recall', 'F1 Score', 'Levenshtein Distance', 'Similarity Ratio'])\n",
    "\n",
    "        for i in range(1,11):\n",
    "            # Read answer file\n",
    "            file_path_answer = path_answer+f'{i}.txt'\n",
    "            with open(file_path_answer, 'r') as file:\n",
    "                answer = file.read()\n",
    "            \n",
    "            # Remove spaces, tabs, and Enter\n",
    "            cleaned_answer = answer.replace(\" \", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            # Read input file\n",
    "            file_path_input = path_input+f'{i}.txt'\n",
    "            with open(file_path_input, 'r') as file:\n",
    "                input = file.read()\n",
    "\n",
    "            # Remove spaces, tabs, and Enter\n",
    "            cleaned_input = input.replace(\" \", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            wer_score = word_error_rate(cleaned_answer, cleaned_input)\n",
    "            cer_score = character_error_rate(cleaned_answer, cleaned_input)\n",
    "            precision, recall, f1_score = precision_recall_f1(cleaned_answer, cleaned_input)\n",
    "            levenshtein_dist = levenshtein_distance(cleaned_answer, cleaned_input)\n",
    "            \n",
    "\n",
    "            print(f\"Image {i}\")\n",
    "            print(f\"Word Error Rate: {wer_score}\")\n",
    "            print(f\"Character Error Rate: {cer_score}\")\n",
    "            print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
    "            print(f\"Levenshtein Distance: {levenshtein_dist}\")\n",
    "            print(f\"Similarity Ratio without SPACE/TAB/ENTER = {similarity_check(cleaned_answer,cleaned_input)*100}\")\n",
    "            \n",
    "            # Write the results to the CSV file\n",
    "            csvwriter.writerow([f\"Image {i}\", wer_score, cer_score, precision, recall, f1_score, levenshtein_dist, similarity_check(answer, input)*100])\n",
    "\n",
    "                        \n",
    "def testing_with(output_csv_path,path_answer,path_input):\n",
    "    # Open the CSV file in write mode\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header row\n",
    "        csvwriter.writerow(['Image', 'Word Error Rate', 'Character Error Rate', 'Precision', 'Recall', 'F1 Score', 'Levenshtein Distance', 'Similarity Ratio'])\n",
    "\n",
    "        for i in range(1,11):\n",
    "            # Read answer file\n",
    "            file_path_answer = path_answer+f'{i}.txt'\n",
    "            with open(file_path_answer, 'r') as file:\n",
    "                answer = file.read()\n",
    "\n",
    "            # Read input file\n",
    "            file_path_input = path_input+f'{i}.txt'\n",
    "            with open(file_path_input, 'r') as file:\n",
    "                input = file.read()\n",
    "\n",
    "            cleaned_answer = answer\n",
    "            cleaned_input = input    \n",
    "            wer_score = word_error_rate(cleaned_answer, cleaned_input)\n",
    "            cer_score = character_error_rate(cleaned_answer, cleaned_input)\n",
    "            precision, recall, f1_score = precision_recall_f1(cleaned_answer, cleaned_input)\n",
    "            levenshtein_dist = levenshtein_distance(cleaned_answer, cleaned_input)\n",
    "            \n",
    "\n",
    "            print(f\"Image {i}\")\n",
    "            print(f\"Word Error Rate: {wer_score}\")\n",
    "            print(f\"Character Error Rate: {cer_score}\")\n",
    "            print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
    "            print(f\"Levenshtein Distance: {levenshtein_dist}\")\n",
    "            print(f\"Similarity Ratio without SPACE/TAB/ENTER = {similarity_check(cleaned_answer,cleaned_input)*100}\")\n",
    "            \n",
    "            # Write the results to the CSV file\n",
    "            csvwriter.writerow([f\"Image {i}\", wer_score, cer_score, precision, recall, f1_score, levenshtein_dist, similarity_check(answer, input)*100])\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Space/Tab/Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_without('./result_without.csv','./answer/','./input/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Space/Tab/Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_with('./result_without.csv','./answer/','./input/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Space/Tab/Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_without('./baseline_without.csv','./answer/','./baseline/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Space/Tab/Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_with('./baseline_without.csv','./answer/','./baseline/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
